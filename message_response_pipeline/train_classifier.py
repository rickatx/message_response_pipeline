# AUTOGENERATED! DO NOT EDIT! File to edit: ML_Pipeline_Preparation.ipynb (unless otherwise specified).

__all__ = ['load_data', 'build_model', 'evaluate_model', 'save_model']

# Cell
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# download nltk data
nltk.download(['stopwords', 'wordnet', 'punkt'])

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
import joblib

# Cell

def load_data(database_filename):
    """Load the classified message data from the specified database engine, and return training data.

    Return:
     - Training input (X), a Series of messages
     - Training targets (Y), a Dataframe of category labels for the training input, with a column
     for each possible category label.
    """
    engine = create_engine(f'sqlite:///{database_filename}')
    with engine.connect() as conn:
        df = pd.read_sql('CategorizedMessages', conn)

    # encode all non-related entries as 0, so we have 1 for related, 0 for non-related
    df_2 = df.copy()
    df_2.related = df_2.related.replace(2, 0)

    # Split into model input and categories
    X = df_2['message']
    Y = df_2.loc[:, 'related':]

    return X, Y

# Internal Cell

stop_words = stopwords.words("english")
lemmatizer = WordNetLemmatizer()
url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

def tokenize(text):
    # convert urls to url token
    text = re.sub(url_regex, 'zzurl', text)

    # normalize case and remove punctuation
    text = re.sub(r"[^a-zA-Z0-9]", " ", text.lower())

    # tokenize text
    tokens = word_tokenize(text)

    # lemmatize and remove stop words
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    return tokens

# Cell

def build_model(estimators=150):
    """Create and return a ML pipeline to train categorizations of text."""
    pipeline = Pipeline([
        ('text_features', TfidfVectorizer(tokenizer=tokenize)),
        ('rfc', MultiOutputClassifier(RandomForestClassifier(), n_jobs=2))
    ])

    return pipeline.set_params(text_features__ngram_range=(1, 2),
                               rfc__estimator__n_estimators=estimators,
                               rfc__estimator__min_samples_split=3)

# Cell

def evaluate_model(trained_model, x_test, y_test):
    """Print a classification report for each of the outputs of a multioutput classification model.

    Args:
     - trained_model: has a predict() method
     - x_test: model inputs from the test set
     - y_test: classifications from the test set (a dataframe with columns naming the outputs)
    """
    predicted = trained_model.predict(x_test)
    col_names = y_test.columns

    for y_true, y_pred, colname in zip(y_test.values.T, predicted.T, col_names):
        print(colname)
        print('='*len(colname))
        print(classification_report(y_true, y_pred))
        print()

# Cell

def save_model(model, model_filepath):
    """Save the trained model, `model` to the specified path, `filepath`."""
    joblib.dump(model, model_filepath)

# Internal Cell

import sys

def main():
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print('Loading data...\n    DATABASE: {}'.format(database_filepath))


        X, Y = load_data(database_filepath)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

        print('Building model...')
        model = build_model()

        print('Training model...')
        model.fit(X_train, Y_train)

        print('Evaluating model...')
        evaluate_model(model, X_test, Y_test)

        print('Saving model...\n    MODEL: {}'.format(model_filepath))
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '\
              'as the first argument and the filepath of the pickle file to '\
              'save the model to as the second argument. \n\nExample: python '\
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


# Don't run main() when this cell is run in a notebook; use try so exported module
# has no dependency on nbdev
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

if __name__ == '__main__' and not IN_NOTEBOOK:
    main()