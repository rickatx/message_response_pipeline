{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# download nltk data\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the data directory, relative to the directory containing this .ipynb\n",
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CleanedMessages.db\n",
      "categories.csv\n",
      "messages.csv\n",
      "test_save.db\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///CleanedMessages.db')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql('CategorizedMessages', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26216 entries, 0 to 26215\n",
      "Data columns (total 40 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   id                      26216 non-null  int64 \n",
      " 1   message                 26216 non-null  object\n",
      " 2   original                10170 non-null  object\n",
      " 3   genre                   26216 non-null  object\n",
      " 4   related                 26216 non-null  int64 \n",
      " 5   request                 26216 non-null  int64 \n",
      " 6   offer                   26216 non-null  int64 \n",
      " 7   aid_related             26216 non-null  int64 \n",
      " 8   medical_help            26216 non-null  int64 \n",
      " 9   medical_products        26216 non-null  int64 \n",
      " 10  search_and_rescue       26216 non-null  int64 \n",
      " 11  security                26216 non-null  int64 \n",
      " 12  military                26216 non-null  int64 \n",
      " 13  child_alone             26216 non-null  int64 \n",
      " 14  water                   26216 non-null  int64 \n",
      " 15  food                    26216 non-null  int64 \n",
      " 16  shelter                 26216 non-null  int64 \n",
      " 17  clothing                26216 non-null  int64 \n",
      " 18  money                   26216 non-null  int64 \n",
      " 19  missing_people          26216 non-null  int64 \n",
      " 20  refugees                26216 non-null  int64 \n",
      " 21  death                   26216 non-null  int64 \n",
      " 22  other_aid               26216 non-null  int64 \n",
      " 23  infrastructure_related  26216 non-null  int64 \n",
      " 24  transport               26216 non-null  int64 \n",
      " 25  buildings               26216 non-null  int64 \n",
      " 26  electricity             26216 non-null  int64 \n",
      " 27  tools                   26216 non-null  int64 \n",
      " 28  hospitals               26216 non-null  int64 \n",
      " 29  shops                   26216 non-null  int64 \n",
      " 30  aid_centers             26216 non-null  int64 \n",
      " 31  other_infrastructure    26216 non-null  int64 \n",
      " 32  weather_related         26216 non-null  int64 \n",
      " 33  floods                  26216 non-null  int64 \n",
      " 34  storm                   26216 non-null  int64 \n",
      " 35  fire                    26216 non-null  int64 \n",
      " 36  earthquake              26216 non-null  int64 \n",
      " 37  cold                    26216 non-null  int64 \n",
      " 38  other_weather           26216 non-null  int64 \n",
      " 39  direct_report           26216 non-null  int64 \n",
      "dtypes: int64(37), object(3)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Verify there are no null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news      13054\n",
       "direct    10766\n",
       "social     2396\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check values of categorization columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    19906\n",
      "0     6122\n",
      "2      188\n",
      "Name: related, dtype: int64\n",
      "\n",
      "0    21742\n",
      "1     4474\n",
      "Name: request, dtype: int64\n",
      "\n",
      "0    26098\n",
      "1      118\n",
      "Name: offer, dtype: int64\n",
      "\n",
      "0    15356\n",
      "1    10860\n",
      "Name: aid_related, dtype: int64\n",
      "\n",
      "0    24132\n",
      "1     2084\n",
      "Name: medical_help, dtype: int64\n",
      "\n",
      "0    24903\n",
      "1     1313\n",
      "Name: medical_products, dtype: int64\n",
      "\n",
      "0    25492\n",
      "1      724\n",
      "Name: search_and_rescue, dtype: int64\n",
      "\n",
      "0    25745\n",
      "1      471\n",
      "Name: security, dtype: int64\n",
      "\n",
      "0    25356\n",
      "1      860\n",
      "Name: military, dtype: int64\n",
      "\n",
      "0    26216\n",
      "Name: child_alone, dtype: int64\n",
      "\n",
      "0    24544\n",
      "1     1672\n",
      "Name: water, dtype: int64\n",
      "\n",
      "0    23293\n",
      "1     2923\n",
      "Name: food, dtype: int64\n",
      "\n",
      "0    23902\n",
      "1     2314\n",
      "Name: shelter, dtype: int64\n",
      "\n",
      "0    25811\n",
      "1      405\n",
      "Name: clothing, dtype: int64\n",
      "\n",
      "0    25612\n",
      "1      604\n",
      "Name: money, dtype: int64\n",
      "\n",
      "0    25918\n",
      "1      298\n",
      "Name: missing_people, dtype: int64\n",
      "\n",
      "0    25341\n",
      "1      875\n",
      "Name: refugees, dtype: int64\n",
      "\n",
      "0    25022\n",
      "1     1194\n",
      "Name: death, dtype: int64\n",
      "\n",
      "0    22770\n",
      "1     3446\n",
      "Name: other_aid, dtype: int64\n",
      "\n",
      "0    24511\n",
      "1     1705\n",
      "Name: infrastructure_related, dtype: int64\n",
      "\n",
      "0    25015\n",
      "1     1201\n",
      "Name: transport, dtype: int64\n",
      "\n",
      "0    24883\n",
      "1     1333\n",
      "Name: buildings, dtype: int64\n",
      "\n",
      "0    25684\n",
      "1      532\n",
      "Name: electricity, dtype: int64\n",
      "\n",
      "0    26057\n",
      "1      159\n",
      "Name: tools, dtype: int64\n",
      "\n",
      "0    25933\n",
      "1      283\n",
      "Name: hospitals, dtype: int64\n",
      "\n",
      "0    26096\n",
      "1      120\n",
      "Name: shops, dtype: int64\n",
      "\n",
      "0    25907\n",
      "1      309\n",
      "Name: aid_centers, dtype: int64\n",
      "\n",
      "0    25065\n",
      "1     1151\n",
      "Name: other_infrastructure, dtype: int64\n",
      "\n",
      "0    18919\n",
      "1     7297\n",
      "Name: weather_related, dtype: int64\n",
      "\n",
      "0    24061\n",
      "1     2155\n",
      "Name: floods, dtype: int64\n",
      "\n",
      "0    23773\n",
      "1     2443\n",
      "Name: storm, dtype: int64\n",
      "\n",
      "0    25934\n",
      "1      282\n",
      "Name: fire, dtype: int64\n",
      "\n",
      "0    23761\n",
      "1     2455\n",
      "Name: earthquake, dtype: int64\n",
      "\n",
      "0    25686\n",
      "1      530\n",
      "Name: cold, dtype: int64\n",
      "\n",
      "0    24840\n",
      "1     1376\n",
      "Name: other_weather, dtype: int64\n",
      "\n",
      "0    21141\n",
      "1     5075\n",
      "Name: direct_report, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[4:]:\n",
    "    print(df[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Classes in 'related' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while most columns have values in {0, 1} indicating false/true, the 'related' column has values from the set: {0, 1, 2}. \n",
    "\n",
    "I didn't find documentation that explained this, so investigate further.\n",
    "\n",
    "What is the character of the messages in each of the categories of the 'related' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= val: 1 =============\n",
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "Is the Hurricane over or is it not over\n",
      "Looking for someone but no name\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "says: west side of Haiti, rest of the country today and tonight\n",
      "Storm at sacred heart of jesus\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "There's nothing to eat and water, we starving and thirsty.\n",
      "I am in Thomassin number 32, in the area named Pyron. I would like to have some water. Thank God we are fine, but we desperately need water. Thanks\n",
      "Let's do it together, need food in Delma 75, in didine area\n",
      "More information on the 4636 number in order for me to participate. ( To see if I can use it )\n",
      "A Comitee in Delmas 19, Rue ( street ) Janvier, Impasse Charite #2. We have about 500 people in a temporary shelter and we are in dire need of Water, Food, Medications, Tents and Clothes. Please stop by and see us.\n",
      "We need food and water in Klecin 12. We are dying of hunger. Impasse Chretien Klecin 12 extended ( extension ) We are hungry and sick.\n",
      "I would like to know if the earthquake is over. Thanks\n",
      "I would like to know if one of the radio ginen Journalist died?\n",
      "I'm in Laplaine, I am a victim\n",
      "There's a lack of water in Moleya, please informed them for me.\n",
      "Those people who live at Sibert need food they are hungry.\n",
      "I want to say hello, my message is to let you know that there's an area in faustin Anhy street that has nothing neither food, water and medicine,\n",
      "People I'm at Delma 2, we don't anything what so ever, please provide us with some food, water, and medicine\n",
      "We are at Gressier we needs assistance right away. ASAP, Come help us.\n",
      "How can we get water and food in Fontamara 43 cite Tinante?\n",
      "We need help. Carrefour has been forgotten completely. The foul odor is killing us. Just letting you know. Thanks!!\n",
      "We have a lot of problem at Delma 75 Avenue Albert Jode, those people need water and food.\n",
      "People have been sleeping outdoors in a field near Lilavois since 12 Jan. No coords on Lilavois but apparently it is near PaP\n",
      "We want you to know that Carrefour need help, we starving to death.\n",
      "How we can find food and water? we have people in many differents needs, and also for medicine at Fontamara 43 cite Tinante.\n",
      "Delmas 33 in Silo, need water.\n",
      "I would like to get help at Cote Plage in Carrefour\n",
      "I am in Leoganes, where can I get food? Please\n",
      "The Comite Miracle in the area of Alerte Rue Monseigneur Guilloux, ( Streets, Alerte and the cross street is Mgr Guilloux ) would like to urgently receive food, water and tents for the people in that area. Thanks\n",
      "People from Dal blocked since Wednesday in Carrefour, we having water shortage, food and medical assistance.\n",
      "In Jacmel it's not working at all, because people are dying of hunger.\n",
      "We have no one everybody is dead.\n",
      "We don't talk about Petit-Goave, Please, look into Petit-Goave.\n",
      "Good evening, is the earthquake end?\n",
      "People in Fontamara 27 in impass area Pierre Louis having lots of difficulties, please help, ASAP.\n",
      "Good evening, I congratulate you for all the good work and with strenght i'm listening from Matisan 1, we have water and food shortage but God provided his grace upon us.\n",
      "how can we find aid and food in fontamara ( carrefour )\n",
      "Am listening to radio in Jacmel. Need help to remove dead bodies at Colege la Trinite-universite, the bodies are the professors and students\n",
      "People from Jacmel are requesting a tractor in that area because there is a civil unrest and social disturbance in that area.\n",
      "Im asking a general prayer for the country of Haiti.\n",
      "We are at KF Marotiere 85, we have food and water shortage, please send food for us.\n",
      "What's up? I would like to know how the Arab is doing.\n",
      "I am listening to radio wa in jacmel. I am asking for help to remove the dead bodies under La trinite college situated at university inasmo. The dead bodies are those of students and teachers\n",
      "In which transfert house does someone abroad has to go to send money to haiti\n",
      "For your information, There are people that are found in the rubbles of the School of Trinite, ( L'Ecole Sainte Trinite ) in Jacmel. Cookies brought to them by Colombian dogs are keeping them alive.\n",
      "When can we get some help in Jacmel?\n",
      "How can we find help and food in fontamara 43 rue menos\n",
      "SOS SOS, please provide police officers on the streets as they are very insecure\n",
      ".. .. . we are here our mother has no more water. if you are there let us know ( location not provided )\n",
      "People needing water with baby Location : Beach 24\n",
      "help is needed in fond parisien, no idea where that is, but any haitian can tell you\n",
      "We're asking for water, medical supply, food\n",
      "My home is at Gressier, it has almost collapsed. I would like to distroy it before it collapses completly.\n",
      "would like to know where food and water are being distributed. ( location not provided )\n",
      "We are seven in the house. Located betwenn Musso and Juvenat. Dyobel-Dyobal. .. not sure The rest of the information is not clear.\n",
      "He is a pastor and an interpreter seeking to help response teams.\n",
      "we have problem at Paco Lazon this is General Police Department we want you to know we almost out of food and water.\n",
      "We are in Sainte Bernadette. There are 2 areas that have not received anything. We are buying water ( warm ) for 5 gourdes.\n",
      "I would like to communicate with my family I don I t have any minutes on my cell phone please help\n",
      "some people are sleeping outside on a field at Lilavois 50 and are dying of hunger since January 12th.\n",
      "Friend I dont know what is going on with you for I hear of what is going on but i have not hear any news about you. I hear what happen Carrefour and petion-Ville\n",
      "i'm asking for help to this valley especially Santo, Sath, B\n",
      "I am a driver, a mechanic ,. I want to help\n",
      "a lot and we don't have any news of him. We haven't even found a body so we could at least know he is dead. The hospitals need to be checked and the registry books also. Please help us!! ( This seems to be the end of another message )\n",
      "We have a big problem in Jacmel, we left Port-au-Prince without food, clothes and money. we are really in a critical situation.\n",
      "We would like to receive some help in the Section Communale. There is a lot of violence.\n",
      "A cold front is found over Cuba this morning. It could cross Haiti tomorrow. Isolated rain showers are expected over our region tonight.\n",
      "Please we need help, food and toiletries.\n",
      "A weather forcast. . of a cold front in cuba and could pass through haiti by night..\n",
      "food water mediacation, we dont have tents to protect our selves, i am using my cistern. lovation not provided\n",
      "The ADJS group house in Jacmel. They are talking with people in the southeast. There are a lot of victims and people suffering, especially in Jacmel. Something about an entire house collapsing. The road sot pop in Jacmel is cut off.\n",
      "This is a request that the aid services not forget the south-east region because there are a lot of victims and people suffering there. A number is given.\n",
      "Where can people in Fort Mercredi can find food and water?\n",
      "( im taking a guess at this sorry its a mix of creole and french. sms.) message as i understand. whos close to mounn fort mercredi  food and water is running low. because there is a problem with people hurt. please help us.\n",
      "We are at Fort Mercredi ( wednesday ) where can we find tents and treated water.\n",
      "whos next to fort mercredi i would like food and water please\n",
      "The wounded in the Saint Michely hospital have no medicine. (This looks like part of a message that got cut off)\n",
      "\n",
      "============= val: 0 =============\n",
      "Information about the National Palace-\n",
      "I would like to receive the messages, thank you\n",
      "I am in Petionville. I need more information regarding 4636\n",
      "are you going to call me or do you want me to call ou? let me know?\n",
      "I don't understand how to use this thing 4636.\n",
      "Can you tell me about this service\n",
      "Good evening, Radio one please. I would like information on Tiyous.\n",
      "I'm here, I didn't find the person that I needed to send the pant by phone\n",
      "I'm listening to you at Miraguan we asking the government to take change because one gallon gas is 80.\n",
      "i am very happy, i hear god, religious hyme\n",
      "I would like to know how food is distributed.\n",
      "I can play carnaval Anbasad Camp there I think this is a simple comment about a creole rap group - Anbasad Camp. Does not refer to location or read as a cry for help. TK\n",
      "ples give us cell cards ( cell phone minuts ) we cannot find cell phone cards to buy\n",
      "Whoever sees her in the streets : she was wearing a white denim skirt and a printed black t-shirt and her hair is in a cornrow do.\n",
      "good people help us. god bless you and us too\n",
      "Digicel let me go through, it's an emergency\n",
      "Where can I find Capital Bank please?\n",
      "URGENT ACTION NEEDED --this is follow up from previous SMS in regards to possible delivery... possible to deliver my baby in an other country, does not matter what it is..\n",
      "Please add airtime on my phone. Thanks\n",
      "City that i'm near is Port-au Prince for me 'm at Titanyen\n",
      "I don't know Rue du Muguet, Route de Desprez 7. Is there another location on the Airport Road?\n",
      "We have a commitee from Santo 15 nameAjeans: Youth Association in action for a new society that was founded in September 2009, the president is..\n",
      "What time will the American Embassy open tomorrow?\n",
      "Is it true that the political city will held in Hinche?\n",
      "in what field would you need me to speak creole. French, half english, ( MW? )\n",
      "Where can I find a car to leave in?\n",
      "Can someone who has a visa travel out. What can he do?\n",
      "I know of an internet service in Croix des Bouquets, Falaise Street. The more important would be to open wireless zones for everyone that has laptops\n",
      "Send me some minutes for my phone\n",
      "Please, what can you do for us. We do not have anything.\n",
      "miques, antihitamines, diapers, folic acid, antifungus, vitamin C, feminine pads, vaginal solutions, klorox, disinfectants.. \n",
      "I come back and nothing is the same my clothes are not there for me to put on\n",
      "Digicel, please put some credit on our phones\n",
      "The manager of digicel phone compagny sent out phone cards for his clients\n",
      "I want to make a call, I can't, there is a crowd of my relatives and I cannot hear, do me a favor please\n",
      "their is someone that left their home and never remembered to call their family, they will never know what happen\n",
      "I was in Port-au-Prince and went to the north-west department. I want to know if internet is working in port-au-prince. Thank you. Let's keep contact\n",
      "I have no card, tell me what kind of SMS I need to send\n",
      "Around my house, there are no cyber cafe for me to use\n",
      "so, in the message you spoke of registering names. I'd like to know what purpose this serves. help me out. thank you\n",
      "I am not in port au prince, I am in Limbe do you know a place where there is working Internet\n",
      "no cybercafe working close to where i live\n",
      "Artists that have died, those who are wounded and those whom have disappeared I will be naming them today, there are more than 6 radios like Radio One, Radio Signal Fm, Maximum Caraib Fm, Lumiere etc\n",
      "Telenet Internet Cafe on Plaisance's old building space hello digicel communication\n",
      "the cyber cafe that is functioning is SERVICE MATICS the address is in front of the BRASSERIE LA COURONNE.\n",
      "K aTIS RAP CREOLE. CONTACT ME FAST. FAST VIA SMS OR CALL URGENT it is very very important because it today im gonna do wats I got to do over the bariers am allready waiting\n",
      "there are no rooms at the mayors office that have internet access\n",
      "this sms is only giving you an address not much detail. . / 2 cyber cafe route of frere. . a litle under where the jumbala night club is..\n",
      "We have the internet at the Internet Cafe by Ti Place Cazeaux\n",
      "MY PHONE HAS NO MINUTE CARD I CANT SEND ANY MESSAGE I WOULD NEED INTERNET I CANT CALL THE PERSON THAT COULD SEND THE MESSAGE.\n",
      "cyber cafe global univercell cabaret 35 km from port au prince\n",
      "i need a card to call my family.\n",
      "the closest cyper cafe is express nety in croix des bouquet befog the police station. TN 4th message i translate advertising a cyper cafe\n",
      "i need a litle credit on my count\n",
      "I don't know the address at all because I just came from Jeremie\n",
      "There's only one cybercafe in the town called MOLE\n",
      "i salut you for all you are doing to help us now. this is actually for the adress of a ciper cafe that is working. i will send it later. TN we had several messages about this yesterday. i suspect this is guy is advertising that he is up and running.\n",
      "The address is new life bo kay in St Fille\n",
      "NET ONE WIT LATORTUE IS FUNCTIONING IN THE ZONE OF DEKAWO. FREDO NET AP F\n",
      "The prices have doubled for food in Mole Saint Nicolas. A cup of Corn is 45 Gdes now.\n",
      "We don't have working cyber in our region\n",
      "hard because everything is sold sky high. Small rice tin can is sold for 40Gourdes 40Gourdes a roughly 1. 00 US\n",
      "there's a problem with the signal which blocks transactions\n",
      "GOOD DAY DIGICEL GIVE ME A CHANCE SO I CA CALL SOMEONE PLEASE\n",
      "GOOD DAY! SWEETHEART I LOVE YOU FOREVER BABY\n",
      "GOOD DAY DIGICEL I DONT KNOW UR WEB ADDRESS\n",
      "HELLO MOM CALL ME BECAUSE I CANNOT GET THROUGH..\n",
      "BRA BALANSE PLEASE THINK ABOUT ME THANK YOU IN ADVANCE\n",
      "I CAN'T GET THROUGH ANY OTHER PHONES BUT DIGICEL\n",
      "I'M IN FRONT OF THE EMBASSY CALL ME\n",
      "please, which cyber cafe is in service? no other information noted within text.\n",
      "can you inform me of any Transfer bureau that is currently functionning?? because i have and urgency\n",
      "self eplanatory. read text msg.\n",
      "Goodday, there is a cybercafe that is available on Route National #1 near the corner of Vincent\n",
      "thank you for the follow up you will give this message and it is important that I tell you that I\n",
      "I speak French very well, I speak Spanish very well, I speak ( no other information )\n",
      "Digicel please i cannot make phone calls, see what you can do for me\n",
      "what cyber cafe's are working?\n",
      "send me the info for the croix des bouquets please\n",
      "I am located in Route Freres- Impasse Fortin. .. A bit further than the Granit Market\n",
      "\n",
      "============= val: 2 =============\n",
      "Dans la zone de Saint Etienne la route de Jacmel est bloqu, il est trsdifficile de se rendre  Jacmel\n",
      ". .. i with limited means. Certain patients come from the capital.\n",
      "The internet caf Net@le that's by the Dal road by the Maranata church ( incomplete )\n",
      "Bonsoir, on est a bon repos aprs la compagnie teleko sur la route a droite de l'impasse Roger colas aprs la 9e maison sur la main droite de la rue, on est environ 30 personnes sur un. ..\n",
      "URGENT CRECHE ORPHANAGE KAY TOUT TIMOUN CROIX DES MISSIONS IMPASSE BALEV BUTTE BOYER MANQUE EAU ET NOURRITURE N ONT VU AUCUN SECOURS DEPUIS 8 JOURS HELP HELP\n",
      "elle est vraiment malade et a besoin d'aide. utilisez mon numero de tlphone pour obtenir plus de renseignements. Nous attendons une reponse. Aucun numero fourni par contre.\n",
      "no authority has passed by to see us. We don't have a place t sleep ( incomplete )\n",
      "It's Over in Gressier. The population in the area - Incomplete\n",
      "we sleep with the baby. Thanks in advance for the help you will bring us. ( incomplete )\n",
      "I need help in Jrmie because I was in Port-au-Prince in university I am counting on you.\n",
      "fsa pou mwen s v p map mouri mwen gen tout po blem ede m a do that for me plz. I'm dying. I have all sorts of problems. Help me\n",
      ".. Gonaives, in a place called Canal Bois in French. i'm writing this message just to.. \n",
      "GEN YON TIBEBE KI MALAD NAN KOU PASKE BLOK TONBE SOU LI. MEN DIREKSYON AN LEW RIVE NAN VIL LEYOGAN WAP MONTE NAN WOUT DABON LEW RIVE NAN KAFOU DABON WAP VIRE NAN BO KKOTI YZIN NAN. WAP JWENN YON KAFOU LEW RIVE PIDEVAN WAP WE SANT CEFECAC LA. OUBYEN LEW NAN WOUT WAP RELEM EPI MAP PRAN YON MOTE POUM VIN CHAC \n",
      "don't understand the first part. .. understanding. We are waiting for your response\n",
      "Est-ce que ya monde qui aller U. s. a et qui ont tomoin qui Americain Are there people who are going to US and that have witness that is american \n",
      "Gens ont information qui dit que si on moins citoyens americains vit ici l que tout aller ok tout le monde vite fil pour qu'on essaie survivre. People have information that say that if american citizens live here that all will be ok and that everyone ( could be the world ) hurrying to try to make us survive.\n",
      "This is my address : Cersine 8 Prolong. .. . \n",
      "I would like to have some information on the last disaster\n",
      "No location : Ou se trouve l'ambassade du Sngal en Haiti\n",
      "Je veux savoir comment je peux obtenir de l'aide pour mon orphelinat situ  Delmas 31, svp dites-moi ce que je dois faire\n",
      "142, Ruelle Beaulieu, Mon Repos 44 Carrefour \n",
      ". ..  Esdras. Address : Solidarit Village, Toussain Louverture Street 83A!\n",
      "Route des dalles prolonges #277 after Morne Jean-Pierre, thank you do. ..  \n",
      "we are in rue Dessalines Petit goaves and it necessary to..? \n",
      "I am very happy to get the messages that tell me about the precautions I have to take. Thank you very much, If you could make it so that food comes my way it would be good because .. \n",
      "mouin vle kite ayiti depi kek jou. \n",
      "Mwen ta renmen jwenn yon djb. ske gen posiblite pou mwen jwenn li? Kisa ou ka f pou ede mwen? Mwen bezwen savis ou. \n",
      "(Delmas 33 Charboniere infomatyon s'il vous plait.) \n",
      "Carrefour Feuille rue Mgr Prol # 39 \n",
      "Ede m nan lort pou genyen yon bourss \n",
      "mwen ta renmen gen enfmasyon sou siklyn nan peyi mwen \n",
      "jel2  Acte 5: 29 2chr7 Lev11 Ecl9 Ecl4 jos5 ch102kr Es59 \n",
      "I'd like to know after this drama, will there be anywhere that I may see a psychologist, who maybe available because I have \n",
      "gjm.adgjmpgjm.adgjmgjmgjmgjmgjmgkmptwptwptwptwptwptwptwptwptwptwpwptwptwgjmgjmgjmad.ad.ad.ad.d.ad.d.da.d.ad.ad.ad \n",
      "are what the imigration restarts has make of them pass-port for people that not of pass-port \n",
      "mwen besoin dlo avek mange na zon bon repos, lilavois \n",
      "Damocles!Hracles!Philockles!Hyphocles.!yayecles!zigzacles!domagecles!lucles!77h  \n",
      "The SMS:  Evitons 2 traiter 1 tas bLaissons 2 cot 7 kesyon. Ke tu sois direct tais notre mach conclus 2 c matin. J't'attends  \n",
      "Midi Estimene:rue Lambert Prolongee and rue Merilus #4 (Petion-ville) \n",
      "Amoni se Richi: Rue Raphael et Cerisier (Petion Ville) \n",
      " 9GeQYeYGQEQtm  \n",
      "annot ni batiman dada cheri se pwason ni ou ni manmi w ni keti renmen an pil la wi se yon gro rat at yon gro kwapo kitap batay lasirne f tranf pou tou \n",
      "0H 8@@ @2x @e8x3 xr     \n",
      "http://wap.sina.comhttp://wap.sina.com \n",
      "Wesantyahoo.fr.Pepayisenyahoo.fr \n",
      "//// // @:@ \n",
      "NOTES: this message is not complete\n",
      "f k yo ap mal viv nan ansagal lagonav.msi.  \n",
      "8 @9q@@  5 m) 1A a!a ?aCYCLONE  \n",
      "/''''''''@''''''''''''''''@ @''''' michou )) ''''''''''''cyclone.  \n",
      "I@UU#UTGa UTUUUXU  UU@UU@UUp U. \n",
      "Good evening to all parts of the United Nations so I remsiw Adra, Spain because I see something to bring 250 people found the four towels soap Cink had a six ice all very much pleased the United Nations kounya other things I'm waiting for luggage next to Always on baylode. \n",
      "Pal kraze preval di pa gen pwoblem, pal jistis kraze preval di sa te dwe kraze 2ja paske pat janm gen jistis, yo di kleren lakay ak rhum barbancourt kraze preval met 2 men nan tt li rele anmwey pa gen peyi ank. \n",
      "0Tranbleman t aH 8@@ @2x @e8x3 xr    ' Gv?e?0   \n",
      "The SMS:  Rezilta Bak la sot bay 12 Janvye a nan lekl la vi a. Men kman l tonbe: Moun ki mouri nan dezobeyisans yo: Elimine Moun ki mouri anba pouvwa Bondye yo: Admi e rs moun ki rete yo ajoune. Pa bliye egzamen moun ajoune yo p'ap two lwen pou'l ft, degaj  \n",
      "Aa.Bb.Cc.Dd.Ee.Ff.Gg.Hh.Ii.Jj.Kk.Ll.Mm.Nn.Oo.Pp.Qq.Rr.Ss.Tt.Uu.Vv.Ww.Xx.Yy.Zz.KERLANGE. \n",
      "3 mamit chabon,4 dola poban,kart,pn 2 t,kreson,tomat. \n",
      "Mato30 Nivo60 Tiwl30 Ficel22 Ek49 \n",
      "Bonjou fr Louko mwen se simon st.pierre map koute radio 1 nan boukan-chat map mandew svp pouw salye moun sayo pou mwen: Eunide Elissain, wiskender st.pierre,et jesual st.pierre,alyas pipirit, kap koute radyo a chapotin. Msi. \n",
      "2O TRI PYE, 5 POIRO, 2GD PESI, 1OMORU, 5LAY3DOLA SITWON \n",
      "Prcaussion cons of the aspen earth \n",
      "Zb rmd : dl 2, dl rep, dl chase \n",
      "Zb rmd : dl c.lve dl 2, dl rep, dl chase. \n",
      "please, we would like to know if itis true thereas in the moment. \n",
      "NOTES: This message is not complete, it is not necessary to translate it.\n",
      "NOTES: This message mark not enough information.\n",
      "Word Hurricane and earthquake \n",
      "Wpltjgdtm pwjtj ptjg. T .)tgjpg(uk \n",
      "Tegen tranblement land kite yon pase nan north country of haiti sete year 1842 Bulan interest unknown \n",
      "0H4Z(Z x(@zf0 ?!?@x PrxP0c6 \n",
      "RT lucasrohan Tem alguma coisa do Haiti? T u00f4 acomapanhando a lista nprnews haiti earthquake t u00e1 ca u00f3tico t u00e1 um horror l u00e1\n",
      "RT paipibat RT Teelek Flickr paipibat u0e15 u0e34 u0e14 u0e15 u0e32 u0e21 u0e20 u0e32 u0e1e u0e08 u0e32 u0e01 u0e40 u0e1e u0e34 u0e48 u0e21 u0e40 u0e15 u0e34 u0e21 u0e08 u0e32 u0e01 Flickr u0e44 u0e14 u0e49 u0e17 u0e35 u0e48 u0e19 u0e35 u0e48 u0e04 u0e23 u0e31 u0e1a http bit.ly 60OxGS #Haiti #Earthquake\n",
      "alhamdulillah sekarang udah jarang Earthquake lagi\n",
      "RT Fleegerian_Akin RT ShanCali Earthquake in carribean I feel sorry 4 dem ppl in hatti da whole island is basi cont http tl.gd 3g8mi\n",
      "Haiti Earthquake Earthquake Hits Haiti C est tre tres rare que trembleme. http bit.ly 53Z4gm n Got News? Add It!\n",
      "RT Contentgirls earthquake heeft mensen wel wakker gekregen maar is niet zwaar. Even benadrukken voor het geval iemand familie heeft o ..\n",
      "Contentgirls gelukkig earthquake niet te zwaar Aruba Antillen. Heb jij er familie?\n",
      "Gismis pallas_ploem earthquake heeft mensen wel wakker gekregen maar is niet zwaar. via Contentgirls\n",
      "RT mHefneratm RT LoSoATM What happened n HaiTi ??&gt Da Whole Situation turn ugly! Xhi Xhi.. Lmao Nah a earthquake! WrD! $hyT Brazzi\n",
      "Cara, eu to m‚àö‚â§ tenso, quero saber se minha prima e minha tia estao bem com esse terremoto '-' elas moram en Santiago no Chile &gt;.&lt;\n"
     ]
    }
   ],
   "source": [
    "for r_val in [1, 0, 2]:\n",
    "    print(f\"\\n============= val: {r_val} =============\")\n",
    "    sub_df = df[df.related==r_val]\n",
    "    for ind in range(80):\n",
    "        print(sub_df.message.iloc[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the 'related' value is 1 if the message is related to some disaster, and 0 otherwise. Messages with 'related' val = 2 include also untranslated messages and miscellaneous garbage.\n",
    "\n",
    "Let's look at the values of the other categorization columns for each of the 3 values for 'related'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.16693459258515"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean number of other flags per row when 'related' col val = 1\n",
    "df[df.related==1].loc[:, 'request':].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean number of other flags per row when 'related' col val = 0\n",
    "df[df.related==0].loc[:, 'request':].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when 'related' col val = 2, none of the other flags are turned on.\n",
    "df[df.related==2].loc[:, 'request':].sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Other categorization flags are on (= 1) for a row only if that row has a value of 1 for 'related'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevalence of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.Udacity.coM/']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(url_regex, \"Some text https://www.Udacity.coM/ more text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(url_regex, 'no url here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of messages with url\n",
    "n_url_msg = 0\n",
    "for msg in df.message:\n",
    "    if re.search(url_regex, msg):\n",
    "        n_url_msg += 1\n",
    "n_url_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_url_related=552, n_url_unrelated=117\n",
      "2.7730332563046316%, 1.8541996830427891%\n"
     ]
    }
   ],
   "source": [
    "# count messages with URLs among messages that are related and not\n",
    "is_related = df.related == 1\n",
    "n_url_related = 0\n",
    "for msg in df[is_related].message:\n",
    "    if re.search(url_regex, msg):\n",
    "        n_url_related += 1\n",
    "        \n",
    "n_url_unrelated = 0\n",
    "for msg in df[~is_related].message:\n",
    "    if re.search(url_regex, msg):\n",
    "        n_url_unrelated += 1\n",
    "        \n",
    "print(f\"n_url_related={n_url_related}, n_url_unrelated={n_url_unrelated}\")\n",
    "print(f\"{100*n_url_related/df[is_related].shape[0]}%, {100*n_url_unrelated/df[~is_related].shape[0]}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of urls in related and unrelated messages are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 70 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# number of unique urls\n",
    "urls = []\n",
    "for msg in df.message:\n",
    "    urls.extend(re.findall(url_regex, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://twitpic.com/16esd9                                                                                        8\n",
       "http://bit.ly/a7zy8s                                                                                             7\n",
       "http://twitpic.com/18deq7                                                                                        6\n",
       "http://twitpic.com/16ad2g                                                                                        6\n",
       "http://twitpic.com/15wu5u                                                                                        5\n",
       "http://bit.ly/cs8BsY                                                                                             3\n",
       "http://blip.fm/                                                                                                  3\n",
       "http://bit.ly/a8pajh                                                                                             3\n",
       "http://172.16.3.136/mymain2.php                                                                                  2\n",
       "http://teeth.com.pk/blog                                                                                         2\n",
       "http://t.co/Sq2ekENu                                                                                             2\n",
       "http://bit.ly/bVYUCM                                                                                             2\n",
       "http://tinyurl.com/yfbwr9e                                                                                       2\n",
       "http://bit.ly/9oNfHA                                                                                             2\n",
       "http://t.co/4OcLhyie                                                                                             2\n",
       "http://172.16.3.136/wap/news-french.php                                                                          2\n",
       "http://reliefweb.int/https://www.hrw.org/news/2013/02/01/mali-malian-army-islamist-groups-executed-prisoners)    2\n",
       "http://wefollow.com                                                                                              2\n",
       "http://bit.ly/9iG2mo                                                                                             2\n",
       "http://t.co/Mp93XaGx                                                                                             2\n",
       "http://bit.ly/cizpk8                                                                                             1\n",
       "http://twitpic.com/184zzz                                                                                        1\n",
       "http://t.co/7dEi6v5u                                                                                             1\n",
       "http://t.co/C42U9QqDlong                                                                                         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_counts = pd.Series(urls).value_counts()\n",
    "url_counts.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Model Input and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle 'related' column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all non-related entries as 0, so we have 1 for related, 0 for non-related\n",
    "df_2 = df.copy()\n",
    "df_2.related = df_2.related.replace(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19906\n",
       "0     6310\n",
       "Name: related, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.related.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into model input and categories\n",
    "X = df_2['message']\n",
    "Y = df_2.loc[:, 'related':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26216, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "Y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # convert urls to url token\n",
    "    text = re.sub(url_regex, 'zzurl', text)\n",
    "    \n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti \n",
      "     ['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n",
      "Is the Hurricane over or is it not over \n",
      "     ['hurricane'] \n",
      "\n",
      "Looking for someone but no name \n",
      "     ['looking', 'someone', 'name'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately. \n",
      "     ['un', 'report', 'leogane', '80', '90', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'need', 'supply', 'desperately'] \n",
      "\n",
      "says: west side of Haiti, rest of the country today and tonight \n",
      "     ['say', 'west', 'side', 'haiti', 'rest', 'country', 'today', 'tonight'] \n",
      "\n",
      "Information about the National Palace- \n",
      "     ['information', 'national', 'palace'] \n",
      "\n",
      "Storm at sacred heart of jesus \n",
      "     ['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "Please, we need tents and water. We are in Silo, Thank you! \n",
      "     ['please', 'need', 'tent', 'water', 'silo', 'thank'] \n",
      "\n",
      "I would like to receive the messages, thank you \n",
      "     ['would', 'like', 'receive', 'message', 'thank'] \n",
      "\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets ) \n",
      "     ['croix', 'de', 'bouquet', 'health', 'issue', 'worker', 'santo', '15', 'area', 'croix', 'de', 'bouquet'] \n",
      "\n",
      "There's nothing to eat and water, we starving and thirsty. \n",
      "     ['nothing', 'eat', 'water', 'starving', 'thirsty'] \n",
      "\n",
      "I am in Petionville. I need more information regarding 4636 \n",
      "     ['petionville', 'need', 'information', 'regarding', '4636'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine tokenize() behavior with some sample strings\n",
    "for msg in X.iloc[:12]:\n",
    "    print(msg, '\\n    ', tokenize(msg), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to find a Job at an NGO or the Government, upload your resume at http://www.jobpaw.com/  \n",
      "     ['want', 'find', 'job', 'ngo', 'government', 'upload', 'resume', 'zzurl'] \n",
      "\n",
      "NOTES: WHAT A JERK ,ALL HAITIANS DONT HAVE ANYTHING TO EAT ,AND ''HE'' ORDERING 3 DAYS WITHOUT FOOD LIKE SUPPORT FOR THOSE WITHOUT FOOD? http://welcome.topuertorico.org/government.shtml \n",
      "     ['note', 'jerk', 'haitian', 'dont', 'anything', 'eat', 'ordering', '3', 'day', 'without', 'food', 'like', 'support', 'without', 'food', 'zzurl'] \n",
      "\n",
      "http://wap.sina.comhttp://wap.sina.com  \n",
      "     ['zzurl'] \n",
      "\n",
      "Nokia.com http://ea.mobile.nokia.com/ea/graphics  \n",
      "     ['nokia', 'com', 'zzurl'] \n",
      "\n",
      "BEGIN:VBKM VERSION:1.0 TITLE:Digicel Live Ha URL:http://172.16.3.136/mymain2.php BEGIN:ENV X-IRMC-URLQUOTED-PRINTABLE: InternetShortcut  URLhttp://172.16.3.136/mymain2.php END:ENV END:VBKM   \n",
      "     ['begin', 'vbkm', 'version', '1', '0', 'title', 'digicel', 'live', 'ha', 'url', 'zzurl', 'begin', 'env', 'x', 'irmc', 'urlquoted', 'printable', 'internetshortcut', 'urlzzurl', 'end', 'env', 'end', 'vbkm'] \n",
      "\n",
      "BEGIN:VBKM VERSION:1.0 TITLE:Item3 URL:http://172.16.3.136/wap/news-french.php#item3 BEGIN:ENV X-IRMC-URLQUOTED-PRINTABLE: InternetShortcut  URLhttp://172.16.3.136/wap/news-french.php#item3 END:ENV END:VBKM  \n",
      "     ['begin', 'vbkm', 'version', '1', '0', 'title', 'item3', 'url', 'zzurl', 'item3', 'begin', 'env', 'x', 'irmc', 'urlquoted', 'printable', 'internetshortcut', 'urlzzurl', 'item3', 'end', 'env', 'end', 'vbkm'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check behavior for string with urls\n",
    "for msg in X.iloc[:10000]:\n",
    "    if re.search(url_regex, msg):\n",
    "        print(msg, '\\n    ', tokenize(msg), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_features', TfidfVectorizer(tokenizer=tokenize)),\n",
    "    ('rfc', MultiOutputClassifier(RandomForestClassifier(), n_jobs=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('text_features',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x00000239E63684C8>,\n",
       "                   use_idf=True, vocabulary=None)),\n",
       "  ('rfc',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=2))],\n",
       " 'verbose': False,\n",
       " 'text_features': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x00000239E63684C8>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       " 'rfc': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=2),\n",
       " 'text_features__analyzer': 'word',\n",
       " 'text_features__binary': False,\n",
       " 'text_features__decode_error': 'strict',\n",
       " 'text_features__dtype': numpy.float64,\n",
       " 'text_features__encoding': 'utf-8',\n",
       " 'text_features__input': 'content',\n",
       " 'text_features__lowercase': True,\n",
       " 'text_features__max_df': 1.0,\n",
       " 'text_features__max_features': None,\n",
       " 'text_features__min_df': 1,\n",
       " 'text_features__ngram_range': (1, 1),\n",
       " 'text_features__norm': 'l2',\n",
       " 'text_features__preprocessor': None,\n",
       " 'text_features__smooth_idf': True,\n",
       " 'text_features__stop_words': None,\n",
       " 'text_features__strip_accents': None,\n",
       " 'text_features__sublinear_tf': False,\n",
       " 'text_features__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'text_features__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'text_features__use_idf': True,\n",
       " 'text_features__vocabulary': None,\n",
       " 'rfc__estimator__bootstrap': True,\n",
       " 'rfc__estimator__ccp_alpha': 0.0,\n",
       " 'rfc__estimator__class_weight': None,\n",
       " 'rfc__estimator__criterion': 'gini',\n",
       " 'rfc__estimator__max_depth': None,\n",
       " 'rfc__estimator__max_features': 'auto',\n",
       " 'rfc__estimator__max_leaf_nodes': None,\n",
       " 'rfc__estimator__max_samples': None,\n",
       " 'rfc__estimator__min_impurity_decrease': 0.0,\n",
       " 'rfc__estimator__min_impurity_split': None,\n",
       " 'rfc__estimator__min_samples_leaf': 1,\n",
       " 'rfc__estimator__min_samples_split': 2,\n",
       " 'rfc__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'rfc__estimator__n_estimators': 100,\n",
       " 'rfc__estimator__n_jobs': None,\n",
       " 'rfc__estimator__oob_score': False,\n",
       " 'rfc__estimator__random_state': None,\n",
       " 'rfc__estimator__verbose': 0,\n",
       " 'rfc__estimator__warm_start': False,\n",
       " 'rfc__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'rfc__n_jobs': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available pipeline params\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train pipeline with chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('text_features',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.8, max_features=10000,\n",
       "                                 min_df=0.0002, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 t...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=3,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=2))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "pipeline.set_params(text_features__max_df=0.8,\n",
    "                    text_features__min_df=2.0/10000,\n",
    "                    text_features__max_features=10000,\n",
    "                    rfc__estimator__n_estimators=100,\n",
    "                    rfc__estimator__min_samples_split=3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping cell\n",
    "predicted_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping cell\n",
    "col_names = Y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.48      0.56      1544\n",
      "           1       0.85      0.93      0.89      5010\n",
      "\n",
      "    accuracy                           0.82      6554\n",
      "   macro avg       0.76      0.70      0.72      6554\n",
      "weighted avg       0.81      0.82      0.81      6554\n",
      "\n",
      "\n",
      "request\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      5456\n",
      "           1       0.83      0.50      0.63      1098\n",
      "\n",
      "    accuracy                           0.90      6554\n",
      "   macro avg       0.87      0.74      0.78      6554\n",
      "weighted avg       0.89      0.90      0.89      6554\n",
      "\n",
      "\n",
      "offer\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6530\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "\n",
      "aid_related\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81      3850\n",
      "           1       0.74      0.71      0.73      2704\n",
      "\n",
      "    accuracy                           0.78      6554\n",
      "   macro avg       0.77      0.77      0.77      6554\n",
      "weighted avg       0.78      0.78      0.78      6554\n",
      "\n",
      "\n",
      "medical_help\n",
      "============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      6056\n",
      "           1       0.62      0.11      0.18       498\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.78      0.55      0.57      6554\n",
      "weighted avg       0.91      0.93      0.90      6554\n",
      "\n",
      "\n",
      "medical_products\n",
      "================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6231\n",
      "           1       0.80      0.14      0.24       323\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.88      0.57      0.61      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "\n",
      "search_and_rescue\n",
      "=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6373\n",
      "           1       0.65      0.11      0.19       181\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.81      0.55      0.59      6554\n",
      "weighted avg       0.97      0.97      0.96      6554\n",
      "\n",
      "\n",
      "security\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6426\n",
      "           1       0.00      0.00      0.00       128\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.49      0.50      0.49      6554\n",
      "weighted avg       0.96      0.98      0.97      6554\n",
      "\n",
      "\n",
      "military\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6357\n",
      "           1       0.64      0.14      0.23       197\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.81      0.57      0.61      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "\n",
      "child_alone\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6554\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       1.00      1.00      1.00      6554\n",
      "weighted avg       1.00      1.00      1.00      6554\n",
      "\n",
      "\n",
      "water\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6168\n",
      "           1       0.86      0.47      0.61       386\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.91      0.73      0.79      6554\n",
      "weighted avg       0.96      0.96      0.96      6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\jptrlab-pip\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "food\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      5833\n",
      "           1       0.80      0.72      0.76       721\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.88      0.85      0.87      6554\n",
      "weighted avg       0.95      0.95      0.95      6554\n",
      "\n",
      "\n",
      "shelter\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      6004\n",
      "           1       0.81      0.48      0.60       550\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.88      0.73      0.79      6554\n",
      "weighted avg       0.94      0.95      0.94      6554\n",
      "\n",
      "\n",
      "clothing\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6451\n",
      "           1       0.84      0.20      0.33       103\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.91      0.60      0.66      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "\n",
      "money\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6398\n",
      "           1       0.75      0.04      0.07       156\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.86      0.52      0.53      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "\n",
      "missing_people\n",
      "==============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6470\n",
      "           1       1.00      0.05      0.09        84\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.99      0.52      0.54      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "\n",
      "refugees\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6333\n",
      "           1       0.61      0.10      0.17       221\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.79      0.55      0.58      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "\n",
      "death\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6269\n",
      "           1       0.84      0.28      0.42       285\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.90      0.64      0.70      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "\n",
      "other_aid\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      5653\n",
      "           1       0.57      0.05      0.10       901\n",
      "\n",
      "    accuracy                           0.86      6554\n",
      "   macro avg       0.72      0.52      0.51      6554\n",
      "weighted avg       0.83      0.86      0.81      6554\n",
      "\n",
      "\n",
      "infrastructure_related\n",
      "======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      6117\n",
      "           1       0.00      0.00      0.00       437\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.47      0.50      0.48      6554\n",
      "weighted avg       0.87      0.93      0.90      6554\n",
      "\n",
      "\n",
      "transport\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6254\n",
      "           1       0.70      0.12      0.21       300\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.83      0.56      0.59      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "\n",
      "buildings\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6221\n",
      "           1       0.78      0.19      0.31       333\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.87      0.59      0.64      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "\n",
      "electricity\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6403\n",
      "           1       0.67      0.01      0.03       151\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.82      0.51      0.51      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "\n",
      "tools\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6504\n",
      "           1       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.99      6554\n",
      "\n",
      "\n",
      "hospitals\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6476\n",
      "           1       0.00      0.00      0.00        78\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "\n",
      "shops\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6527\n",
      "           1       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "\n",
      "aid_centers\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6473\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "\n",
      "other_infrastructure\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6264\n",
      "           1       0.00      0.00      0.00       290\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.48      0.50      0.49      6554\n",
      "weighted avg       0.91      0.95      0.93      6554\n",
      "\n",
      "\n",
      "weather_related\n",
      "===============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      4667\n",
      "           1       0.84      0.73      0.78      1887\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.87      0.84      0.85      6554\n",
      "weighted avg       0.88      0.88      0.88      6554\n",
      "\n",
      "\n",
      "floods\n",
      "======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6034\n",
      "           1       0.90      0.53      0.67       520\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.93      0.76      0.82      6554\n",
      "weighted avg       0.96      0.96      0.95      6554\n",
      "\n",
      "\n",
      "storm\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      5927\n",
      "           1       0.75      0.63      0.69       627\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.86      0.80      0.83      6554\n",
      "weighted avg       0.94      0.94      0.94      6554\n",
      "\n",
      "\n",
      "fire\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6491\n",
      "           1       1.00      0.11      0.20        63\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       1.00      0.56      0.60      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "\n",
      "earthquake\n",
      "==========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5881\n",
      "           1       0.89      0.82      0.86       673\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.94      0.91      0.92      6554\n",
      "weighted avg       0.97      0.97      0.97      6554\n",
      "\n",
      "\n",
      "cold\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6442\n",
      "           1       0.69      0.20      0.31       112\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.84      0.60      0.65      6554\n",
      "weighted avg       0.98      0.98      0.98      6554\n",
      "\n",
      "\n",
      "other_weather\n",
      "=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6176\n",
      "           1       0.49      0.06      0.10       378\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.72      0.53      0.54      6554\n",
      "weighted avg       0.92      0.94      0.92      6554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "direct_report\n",
      "=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92      5257\n",
      "           1       0.81      0.37      0.50      1297\n",
      "\n",
      "    accuracy                           0.86      6554\n",
      "   macro avg       0.84      0.67      0.71      6554\n",
      "weighted avg       0.85      0.86      0.84      6554\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "for y_true, y_pred, colname in zip(Y_test.values.T, predicted_test.T, col_names):\n",
    "    print(colname)\n",
    "    print('='*len(colname))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore values for ngram and max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for grid search\n",
    "cv_params = {\n",
    "    'text_features__ngram_range': ((1, 1), (1, 2)),\n",
    "    'text_features__max_df': (0.8,), #(0.5, 0.75, 1.0),\n",
    "    'text_features__max_features': (10000, None), #(5000, 10000, None),\n",
    "    'text_features__use_idf': (True,), # (True, False),\n",
    "    'rfc__estimator__n_estimators': [100], # [50, 100, 200],\n",
    "    'rfc__estimator__min_samples_split': [3], # [2, 3, 4]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 1min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('text_features',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.8,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=0.0002,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_wor...\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'rfc__estimator__min_samples_split': [3],\n",
       "                         'rfc__estimator__n_estimators': [100],\n",
       "                         'text_features__max_df': (0.8,),\n",
       "                         'text_features__max_features': (10000, None),\n",
       "                         'text_features__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'text_features__use_idf': (True,)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "%%time\n",
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__estimator__min_samples_split': 3,\n",
       " 'rfc__estimator__n_estimators': 100,\n",
       " 'text_features__max_df': 0.8,\n",
       " 'text_features__max_features': None,\n",
       " 'text_features__ngram_range': (1, 2),\n",
       " 'text_features__use_idf': True}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rfc__estimator__min_samples_split</th>\n",
       "      <th>param_rfc__estimator__n_estimators</th>\n",
       "      <th>param_text_features__max_df</th>\n",
       "      <th>param_text_features__max_features</th>\n",
       "      <th>param_text_features__ngram_range</th>\n",
       "      <th>param_text_features__use_idf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149.089800</td>\n",
       "      <td>2.333451</td>\n",
       "      <td>11.227994</td>\n",
       "      <td>0.461426</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.290364</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.277467</td>\n",
       "      <td>0.281790</td>\n",
       "      <td>0.268820</td>\n",
       "      <td>0.281659</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.480633</td>\n",
       "      <td>1.983064</td>\n",
       "      <td>11.380506</td>\n",
       "      <td>0.100678</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.293415</td>\n",
       "      <td>0.280010</td>\n",
       "      <td>0.286368</td>\n",
       "      <td>0.276195</td>\n",
       "      <td>0.286898</td>\n",
       "      <td>0.008238</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147.773228</td>\n",
       "      <td>1.111228</td>\n",
       "      <td>10.908775</td>\n",
       "      <td>0.320882</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.294940</td>\n",
       "      <td>0.291889</td>\n",
       "      <td>0.274161</td>\n",
       "      <td>0.288657</td>\n",
       "      <td>0.263733</td>\n",
       "      <td>0.282676</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181.596993</td>\n",
       "      <td>11.550008</td>\n",
       "      <td>13.276466</td>\n",
       "      <td>2.423728</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.299263</td>\n",
       "      <td>0.302059</td>\n",
       "      <td>0.284842</td>\n",
       "      <td>0.293489</td>\n",
       "      <td>0.277213</td>\n",
       "      <td>0.291373</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     149.089800      2.333451        11.227994        0.461426   \n",
       "1     161.480633      1.983064        11.380506        0.100678   \n",
       "2     147.773228      1.111228        10.908775        0.320882   \n",
       "3     181.596993     11.550008        13.276466        2.423728   \n",
       "\n",
       "  param_rfc__estimator__min_samples_split param_rfc__estimator__n_estimators  \\\n",
       "0                                       3                                100   \n",
       "1                                       3                                100   \n",
       "2                                       3                                100   \n",
       "3                                       3                                100   \n",
       "\n",
       "  param_text_features__max_df param_text_features__max_features  \\\n",
       "0                         0.8                             10000   \n",
       "1                         0.8                             10000   \n",
       "2                         0.8                              None   \n",
       "3                         0.8                              None   \n",
       "\n",
       "  param_text_features__ngram_range param_text_features__use_idf  \\\n",
       "0                           (1, 1)                         True   \n",
       "1                           (1, 2)                         True   \n",
       "2                           (1, 1)                         True   \n",
       "3                           (1, 2)                         True   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.290364   \n",
       "1  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.298500   \n",
       "2  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.294940   \n",
       "3  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.299263   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.289855           0.277467           0.281790           0.268820   \n",
       "1           0.293415           0.280010           0.286368           0.276195   \n",
       "2           0.291889           0.274161           0.288657           0.263733   \n",
       "3           0.302059           0.284842           0.293489           0.277213   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.281659        0.008067                4  \n",
       "1         0.286898        0.008238                2  \n",
       "2         0.282676        0.011852                3  \n",
       "3         0.291373        0.009210                1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "pd.DataFrame(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore parameter values for max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'text_features__ngram_range': ((1, 2),),\n",
    "    'text_features__max_df': (0.8, 0.9, 1.0), #(0.5, 0.75, 1.0),\n",
    "    'text_features__max_features': (None,), #(5000, 10000, None),\n",
    "    'rfc__estimator__n_estimators': [100], # [50, 100, 200],\n",
    "    'rfc__estimator__min_samples_split': [3] # [2, 3, 4]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=cv_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('text_features',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.8,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=0.0002,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=...\n",
       "                                                                                               warm_start=False),\n",
       "                                                              n_jobs=2))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'rfc__estimator__min_samples_split': [3],\n",
       "                         'rfc__estimator__n_estimators': [100],\n",
       "                         'text_features__max_df': (0.8, 0.9, 1.0),\n",
       "                         'text_features__max_features': (None,),\n",
       "                         'text_features__ngram_range': ((1, 2),)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "%%time\n",
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__estimator__min_samples_split': 3,\n",
       " 'rfc__estimator__n_estimators': 100,\n",
       " 'text_features__max_df': 1.0,\n",
       " 'text_features__max_features': None,\n",
       " 'text_features__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rfc__estimator__min_samples_split</th>\n",
       "      <th>param_rfc__estimator__n_estimators</th>\n",
       "      <th>param_text_features__max_df</th>\n",
       "      <th>param_text_features__max_features</th>\n",
       "      <th>param_text_features__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144.819614</td>\n",
       "      <td>6.677293</td>\n",
       "      <td>16.512791</td>\n",
       "      <td>0.834994</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.287611</td>\n",
       "      <td>0.285627</td>\n",
       "      <td>0.276625</td>\n",
       "      <td>0.283288</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156.813878</td>\n",
       "      <td>13.763455</td>\n",
       "      <td>16.579252</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.288526</td>\n",
       "      <td>0.290510</td>\n",
       "      <td>0.276167</td>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.639427</td>\n",
       "      <td>4.676143</td>\n",
       "      <td>17.892474</td>\n",
       "      <td>1.905811</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.289136</td>\n",
       "      <td>0.287305</td>\n",
       "      <td>0.278761</td>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     144.819614      6.677293        16.512791        0.834994   \n",
       "1     156.813878     13.763455        16.579252        0.697900   \n",
       "2     157.639427      4.676143        17.892474        1.905811   \n",
       "\n",
       "  param_rfc__estimator__min_samples_split param_rfc__estimator__n_estimators  \\\n",
       "0                                       3                                100   \n",
       "1                                       3                                100   \n",
       "2                                       3                                100   \n",
       "\n",
       "  param_text_features__max_df param_text_features__max_features  \\\n",
       "0                         0.8                              None   \n",
       "1                         0.9                              None   \n",
       "2                           1                              None   \n",
       "\n",
       "  param_text_features__ngram_range  \\\n",
       "0                           (1, 2)   \n",
       "1                           (1, 2)   \n",
       "2                           (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.287611   \n",
       "1  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.288526   \n",
       "2  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.289136   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.285627           0.276625         0.283288        0.004780   \n",
       "1           0.290510           0.276167         0.285068        0.006345   \n",
       "2           0.287305           0.278761         0.285068        0.004522   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                2  \n",
       "2                1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "pd.DataFrame(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore parameter values for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for grid search\n",
    "cv_params = {\n",
    "    'text_features__ngram_range': ((1, 2),),\n",
    "    'rfc__estimator__n_estimators': [100, 150], # [50, 100, 200],\n",
    "    'rfc__estimator__min_samples_split': [3, 4], # [2, 3, 4]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=cv_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 51min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('text_features',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        s...\n",
       "                                                                                               n_estimators=100,\n",
       "                                                                                               n_jobs=None,\n",
       "                                                                                               oob_score=False,\n",
       "                                                                                               random_state=None,\n",
       "                                                                                               verbose=0,\n",
       "                                                                                               warm_start=False),\n",
       "                                                              n_jobs=2))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'rfc__estimator__min_samples_split': [3, 4],\n",
       "                         'rfc__estimator__n_estimators': [100, 150],\n",
       "                         'text_features__ngram_range': ((1, 2),)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "%%time\n",
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rfc__estimator__min_samples_split</th>\n",
       "      <th>param_rfc__estimator__n_estimators</th>\n",
       "      <th>param_text_features__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>351.798884</td>\n",
       "      <td>8.290344</td>\n",
       "      <td>20.927846</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.251755</td>\n",
       "      <td>0.256332</td>\n",
       "      <td>0.267470</td>\n",
       "      <td>0.258519</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>524.421279</td>\n",
       "      <td>10.989128</td>\n",
       "      <td>29.991192</td>\n",
       "      <td>0.316006</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 3, 'rfc_...</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.263656</td>\n",
       "      <td>0.268843</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>320.557537</td>\n",
       "      <td>6.102964</td>\n",
       "      <td>20.622887</td>\n",
       "      <td>0.190247</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 4, 'rfc_...</td>\n",
       "      <td>0.253128</td>\n",
       "      <td>0.257553</td>\n",
       "      <td>0.262740</td>\n",
       "      <td>0.257807</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529.091756</td>\n",
       "      <td>38.520307</td>\n",
       "      <td>45.809671</td>\n",
       "      <td>22.776831</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'rfc__estimator__min_samples_split': 4, 'rfc_...</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>0.270980</td>\n",
       "      <td>0.263249</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     351.798884      8.290344        20.927846        0.190404   \n",
       "1     524.421279     10.989128        29.991192        0.316006   \n",
       "2     320.557537      6.102964        20.622887        0.190247   \n",
       "3     529.091756     38.520307        45.809671       22.776831   \n",
       "\n",
       "  param_rfc__estimator__min_samples_split param_rfc__estimator__n_estimators  \\\n",
       "0                                       3                                100   \n",
       "1                                       3                                150   \n",
       "2                                       4                                100   \n",
       "3                                       4                                150   \n",
       "\n",
       "  param_text_features__ngram_range  \\\n",
       "0                           (1, 2)   \n",
       "1                           (1, 2)   \n",
       "2                           (1, 2)   \n",
       "3                           (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.251755   \n",
       "1  {'rfc__estimator__min_samples_split': 3, 'rfc_...           0.257400   \n",
       "2  {'rfc__estimator__min_samples_split': 4, 'rfc_...           0.253128   \n",
       "3  {'rfc__estimator__min_samples_split': 4, 'rfc_...           0.252212   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.256332           0.267470         0.258519        0.006600   \n",
       "1           0.263656           0.268843         0.263300        0.004679   \n",
       "2           0.257553           0.262740         0.257807        0.003928   \n",
       "3           0.266555           0.270980         0.263249        0.008010   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                1  \n",
       "2                4  \n",
       "3                2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "pd.DataFrame(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multioutput_classification_report(trained_model, x_test, y_test):\n",
    "    \"\"\"Print a classification report for each of the outputs of a multioutput classification model.\n",
    "    \n",
    "    Args:\n",
    "     - trained_model: has a predict() method\n",
    "     - x_test: model inputs from the test set\n",
    "     - y_test: model outputs from the test set (a dataframe with columns naming the outputs)\n",
    "    \"\"\"\n",
    "    predicted = trained_model.predict(x_test)\n",
    "    col_names = y_test.columns\n",
    "    \n",
    "    for y_true, y_pred, colname in zip(y_test.values.T, predicted.T, col_names):\n",
    "        print(colname)\n",
    "        print('='*len(colname))\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.46      0.56      1603\n",
      "           1       0.84      0.93      0.89      4951\n",
      "\n",
      "    accuracy                           0.82      6554\n",
      "   macro avg       0.77      0.70      0.72      6554\n",
      "weighted avg       0.81      0.82      0.81      6554\n",
      "\n",
      "\n",
      "request\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      5470\n",
      "           1       0.81      0.50      0.62      1084\n",
      "\n",
      "    accuracy                           0.90      6554\n",
      "   macro avg       0.86      0.74      0.78      6554\n",
      "weighted avg       0.89      0.90      0.89      6554\n",
      "\n",
      "\n",
      "offer\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6525\n",
      "           1       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "\n",
      "aid_related\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      3872\n",
      "           1       0.77      0.69      0.73      2682\n",
      "\n",
      "    accuracy                           0.79      6554\n",
      "   macro avg       0.78      0.77      0.78      6554\n",
      "weighted avg       0.79      0.79      0.79      6554\n",
      "\n",
      "\n",
      "medical_help\n",
      "============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      6045\n",
      "           1       0.67      0.07      0.13       509\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.80      0.53      0.54      6554\n",
      "weighted avg       0.91      0.93      0.90      6554\n",
      "\n",
      "\n",
      "medical_products\n",
      "================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6217\n",
      "           1       0.78      0.11      0.19       337\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.87      0.55      0.58      6554\n",
      "weighted avg       0.94      0.95      0.94      6554\n",
      "\n",
      "\n",
      "search_and_rescue\n",
      "=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6366\n",
      "           1       0.84      0.09      0.15       188\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.91      0.54      0.57      6554\n",
      "weighted avg       0.97      0.97      0.96      6554\n",
      "\n",
      "\n",
      "security\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6429\n",
      "           1       0.00      0.00      0.00       125\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.96      0.98      0.97      6554\n",
      "\n",
      "\n",
      "military\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6343\n",
      "           1       0.60      0.03      0.05       211\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.78      0.51      0.52      6554\n",
      "weighted avg       0.96      0.97      0.95      6554\n",
      "\n",
      "\n",
      "child_alone\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6554\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       1.00      1.00      1.00      6554\n",
      "weighted avg       1.00      1.00      1.00      6554\n",
      "\n",
      "\n",
      "water\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6134\n",
      "           1       0.85      0.42      0.56       420\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.91      0.71      0.77      6554\n",
      "weighted avg       0.95      0.96      0.95      6554\n",
      "\n",
      "\n",
      "food\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      5814\n",
      "           1       0.84      0.61      0.70       740\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.89      0.80      0.84      6554\n",
      "weighted avg       0.94      0.94      0.94      6554\n",
      "\n",
      "\n",
      "shelter\n",
      "=======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5957\n",
      "           1       0.83      0.34      0.48       597\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.88      0.66      0.72      6554\n",
      "weighted avg       0.93      0.93      0.92      6554\n",
      "\n",
      "\n",
      "clothing\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6447\n",
      "           1       0.83      0.09      0.17       107\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.91      0.55      0.58      6554\n",
      "weighted avg       0.98      0.98      0.98      6554\n",
      "\n",
      "\n",
      "money\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6382\n",
      "           1       0.86      0.03      0.07       172\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.92      0.52      0.53      6554\n",
      "weighted avg       0.97      0.97      0.96      6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\jptrlab-pip\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "missing_people\n",
      "==============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6468\n",
      "           1       1.00      0.01      0.02        86\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.99      0.51      0.51      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "\n",
      "refugees\n",
      "========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6327\n",
      "           1       0.91      0.04      0.08       227\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.94      0.52      0.53      6554\n",
      "weighted avg       0.96      0.97      0.95      6554\n",
      "\n",
      "\n",
      "death\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6270\n",
      "           1       0.87      0.19      0.31       284\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.92      0.59      0.65      6554\n",
      "weighted avg       0.96      0.96      0.95      6554\n",
      "\n",
      "\n",
      "other_aid\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93      5732\n",
      "           1       0.55      0.03      0.05       822\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.71      0.51      0.49      6554\n",
      "weighted avg       0.84      0.88      0.82      6554\n",
      "\n",
      "\n",
      "infrastructure_related\n",
      "======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      6121\n",
      "           1       0.15      0.00      0.01       433\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.54      0.50      0.49      6554\n",
      "weighted avg       0.88      0.93      0.90      6554\n",
      "\n",
      "\n",
      "transport\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6238\n",
      "           1       0.86      0.04      0.07       316\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.91      0.52      0.52      6554\n",
      "weighted avg       0.95      0.95      0.93      6554\n",
      "\n",
      "\n",
      "buildings\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6224\n",
      "           1       0.77      0.17      0.27       330\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.87      0.58      0.63      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "\n",
      "electricity\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6422\n",
      "           1       0.50      0.02      0.04       132\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.74      0.51      0.52      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "\n",
      "tools\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6517\n",
      "           1       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "\n",
      "hospitals\n",
      "=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6490\n",
      "           1       0.00      0.00      0.00        64\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.99      6554\n",
      "\n",
      "\n",
      "shops\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6523\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "\n",
      "aid_centers\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6463\n",
      "           1       0.00      0.00      0.00        91\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.97      0.99      0.98      6554\n",
      "\n",
      "\n",
      "other_infrastructure\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6261\n",
      "           1       0.00      0.00      0.00       293\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.48      0.50      0.49      6554\n",
      "weighted avg       0.91      0.95      0.93      6554\n",
      "\n",
      "\n",
      "weather_related\n",
      "===============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      4738\n",
      "           1       0.87      0.65      0.74      1816\n",
      "\n",
      "    accuracy                           0.87      6554\n",
      "   macro avg       0.87      0.80      0.83      6554\n",
      "weighted avg       0.87      0.87      0.87      6554\n",
      "\n",
      "\n",
      "floods\n",
      "======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6016\n",
      "           1       0.91      0.41      0.57       538\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.93      0.71      0.77      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "\n",
      "storm\n",
      "=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5950\n",
      "           1       0.79      0.36      0.49       604\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.86      0.68      0.73      6554\n",
      "weighted avg       0.92      0.93      0.92      6554\n",
      "\n",
      "\n",
      "fire\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6486\n",
      "           1       0.50      0.01      0.03        68\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.74      0.51      0.51      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "\n",
      "earthquake\n",
      "==========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5934\n",
      "           1       0.90      0.79      0.84       620\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.94      0.89      0.91      6554\n",
      "weighted avg       0.97      0.97      0.97      6554\n",
      "\n",
      "\n",
      "cold\n",
      "====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6429\n",
      "           1       0.44      0.03      0.06       125\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.71      0.52      0.52      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "\n",
      "other_weather\n",
      "=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6236\n",
      "           1       0.61      0.05      0.10       318\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.78      0.53      0.54      6554\n",
      "weighted avg       0.94      0.95      0.93      6554\n",
      "\n",
      "\n",
      "direct_report\n",
      "=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      5321\n",
      "           1       0.79      0.38      0.51      1233\n",
      "\n",
      "    accuracy                           0.86      6554\n",
      "   macro avg       0.83      0.68      0.72      6554\n",
      "weighted avg       0.86      0.86      0.84      6554\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "multioutput_classification_report(cv, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
